{"id": "123", "text": "One of the fundamental assumptions for machine-learning based text classification systems is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. However, in live news aggregation sites, this assumption is rarely correct. Instead, the events and topics discussed in news stories dramatically change over time. Rather than ignoring this phenomenon, we attempt to explicitly model the transitions of news stories and classifications over time to label stories that may be acquired months after the initial examples are labeled. We test our system, based on efficiently propagating labels in time-based graphs, with recently published news stories collected over an eighty day period. Experiments presented in this paper include the use of training labels from each story within the first several days of gathering stories, to using a single story as a label.\nThe writing, vocabulary, and topic of news stories rapidly shift within extremely small periods of time. In recent years, new events and breaking, \u201chot\u201d, stories almost instantaneously dominate the majority of the press, while older topics just as quickly recede from popularity. For typical automated news-classification systems, this can present severe challenges. For example, the \u2018Political\u2019 and \u2018Entertainment\u2019 breaking news stories of one week may have very little in common, in terms of subject or even vocabulary, with the news stories of the next week. An automated news classifier that is trained to accurately recognize the previous day/month/year\u2019s stories may not have encountered the type of news story that will arise tomorrow.\nUnlike previous work on topic detection and tracking, we are not attempting to follow a particular topic over time or to determine when a new topic has emerged. Instead, we are addressing a related problem of immediate interest to live news aggregation sites: given that a news story has been published, in which of the site\u2019s preset categories should it be placed?\nThe volume of news stories necessitates the use of an automated classifier. However, one of the fundamental assumptions in machine learning based approaches to news classification is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. Because of the rapidly changing nature of news stories, this may not hold true. In this paper, we present a graph-based\napproach to address the problem of explicitly capturing both strong and weak similarities within news stories over time and to use these efficiently for categorization. Our approach combines the paradigm of Min-Hashing and label propagation in graphs in a novel way. While Min-Hashing is well-understood in information retrieval applications, our application of it to create a temporal similarity graph appears to be new. Label propagation is gaining popularity in the field of machine learning as a technique for semi- supervised learning. Our approach to label propagation follows our previous work [4], where equivalent views of a basic algorithm termed Adsorption were established, and the technique was successfully employed for propagating weak information in extremely large graphs to create a video recommendation system for YouTube.\nThe aims of this paper are to present the following techniques that we anticipate will have general applicability for data mining in industrial settings: formulation of temporal similarities via graphs created using Min-Hashes, and the application of label propagation as an off-the-shelf tool for classification tasks when very little ground truth is available.\nThe next section describes the data collected and presents a series of experiments to develop strong, realistic, baselines for performance. Section 3 gives a detailed description of the Adsorption algorithm. Section 4 presents the empirical results to establish the Adsorption baselines for this task. Section 5 presents extensive results with tiny amounts of labeled data (e.g., a single labeled example). Section 6 concludes the paper and offers avenues for future exploration."}