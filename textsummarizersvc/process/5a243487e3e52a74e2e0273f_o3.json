{"dist": 0.029716259363208365, "idx": 15, "text": "The second LSTM is essentially a recurrent neural network language model \n [ 28 , 23 , 30 ] except that it is conditioned on the input sequence ."}
{"dist": 0.02386764413619858, "idx": 18, "text": "There have been a number of related attempts to address the general sequence to sequence learning \n problem with neural networks ."}
{"dist": 0.021257915972655136, "idx": 8, "text": "For example , speech recognition and machine translation are sequential problems ."}
{"dist": 0.02119285019542333, "idx": 22, "text": "The Connectionist Sequence Classification is another popular \n technique for mapping sequences to sequences with neural networks , but it assumes a monotonic \n alignment between the inputs and the outputs [ 11 ] ."}
{"dist": 0.020518299669064457, "idx": 7, "text": "It is a significant limitation , since \n many important problems are best expressed with sequences whose lengths are not known a-priori . \n"}
{"dist": 0.019769244078665475, "idx": 11, "text": "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and \n outputs is known and fixed ."}
{"dist": 0.01933736578452119, "idx": 25, "text": "This is by far the best result achieved by direct translation with large neural networks . \n"}
{"dist": 0.018942854535200653, "idx": 12, "text": "In this paper , we show that a straightforward application of the Long \n Short-Term Memory ( LSTM ) architecture [ 16 ] can solve general sequence to sequence problems . \n"}
{"dist": 0.01794498212068065, "idx": 36, "text": "A useful property of the LSTM is that it learns to map an input sentence of variable length into \n a fixed-dimensional vector representation ."}
{"dist": 0.016923778322067574, "idx": 0, "text": "Deep Neural Networks ( DNNs ) are extremely powerful machine learning models that achieve excellent \n performance on difficult problems such as speech recognition [ 13 , 7 ] and visual object recognition \n [ 19 , 6 , 21 , 20 ] ."}
{"dist": 0.01651715102988129, "idx": 10, "text": "It is therefore clear that a domain-independent method \n that learns to map sequences to sequences would be useful . \n"}
{"dist": 0.016250801327555357, "idx": 3, "text": "So , while neural networks are \n related to conventional statistical models , they learn an intricate computation ."}
{"dist": 0.015749945815215247, "idx": 9, "text": "Likewise , question \n answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer ."}
{"dist": 0.015245425325177217, "idx": 6, "text": "Despite their flexibility and power , DNNs can only be applied to problems whose inputs and targets \n can be sensibly encoded with vectors of fixed dimensionality ."}
{"dist": 0.015139129862648995, "idx": 13, "text": "The idea is to use one LSTM to read the input sequence , one timestep at a time , to obtain large fixeddimensional \n vector representation , and then to use another LSTM to extract the output sequence \n from that vector ( fig ."}
{"dist": 0.014838876271444586, "idx": 34, "text": "As a result , SGD could learn LSTMs that had no trouble with \n long sentences ."}
{"dist": 0.013840434905310457, "idx": 32, "text": "We were able to do well on long sentences because we \n reversed the order of words in the source sentence but not the target sentences in the training and test \n set ."}
{"dist": 0.013376130312588084, "idx": 19, "text": "Our approach is closely related to Kalchbrenner and Blunsom [ 18 ] \n who were the first to map the entire input sentence to vector , and is related to Cho et al . [ 5 ] although \n the latter was used only for rescoring hypotheses produced by a phrase-based system ."}
{"dist": 0.013348005147187196, "idx": 28, "text": "This result shows \n that a relatively unoptimized small-vocabulary neural network architecture which has much room \n for improvement outperforms a phrase-based SMT system . \n"}
{"dist": 0.013011090432124974, "idx": 20, "text": "Graves [ 10 ] \n introduced a novel differentiable attention mechanism that allows neural networks to focus on different \n parts of their input , and an elegant variant of this idea was successfully applied to machine \n translation by Bahdanau et al ."}
{"dist": 0.012657460891396228, "idx": 31, "text": "Surprisingly , the LSTM did not suffer on very long sentences , despite the recent experience of other \n researchers with related architectures [ 26 ] ."}
{"dist": 0.012343583092925235, "idx": 16, "text": "The LSTM \u2019s ability to successfully \n learn on data with long range temporal dependencies makes it a natural choice for this application \n due to the considerable time lag between the inputs and their corresponding outputs ( fig ."}
{"dist": 0.01216412509940493, "idx": 35, "text": "The simple trick of reversing the words in the source sentence is one of the key \n technical contributions of this work . \n"}
{"dist": 0.011620402691210688, "idx": 23, "text": "The main result of this work is the following ."}
{"dist": 0.011533191722685363, "idx": 37, "text": "Given that translations tend to be paraphrases of the \n source sentences , the translation objective encourages the LSTM to find sentence representations \n that capture their meaning , as sentences with similar meanings are close to each other while different sentences meanings will be far ."}
{"dist": 0.010105374353120467, "idx": 33, "text": "By doing so , we introduced many short term dependencies that made the optimization problem \n much simpler ( see sec . 2 and 3.3 ) ."}
{"dist": 0.010066551835358683, "idx": 5, "text": "Thus , if there exists a parameter setting of a large \n DNN that achieves good results ( for example , because humans can solve the task very rapidly ) , \n supervised backpropagation will find these parameters and solve the problem . \n"}
{"dist": 0.009873497358454775, "idx": 26, "text": "For comparison , the BLEU score of an SMT baseline on this dataset is 33.30 [ 29 ] ."}
{"dist": 0.009440916337466644, "idx": 27, "text": "The 34.81 \n BLEU score was achieved by an LSTM with a vocabulary of 80k words , so the score was penalized \n whenever the reference translation contained a word not covered by these 80k ."}
{"dist": 0.008683810027160975, "idx": 4, "text": "Furthermore , large \n DNNs can be trained with supervised backpropagation whenever the labeled training set has enough \n information to specify the network \u2019s parameters ."}
{"dist": 0.00791578030947658, "idx": 29, "text": "Finally , we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on \n the same task [ 29 ] ."}
{"dist": 0.0076497006615789415, "idx": 1, "text": "DNNs are powerful because they can perform arbitrary parallel computation \n for a modest number of steps ."}
{"dist": 0.0072687130086834225, "idx": 30, "text": "By doing so , we obtained a BLEU score of 36.5 , which improves the baseline by \n 3.2 BLEU points and is close to the previous best published result on this task ( which is 37.0 [ 9 ] ) . \n"}
{"dist": 0.006858695132063723, "idx": 38, "text": "A qualitative evaluation supports this claim , showing that our model \n is aware of word order and is fairly invariant to the active and passive voice ."}
{"dist": 0.006723536688936776, "idx": 24, "text": "On the WMT\u201914 English to French translation task , \n we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep \n LSTMs ( with 384 M parameters and 8,000 dimensional state each ) using a simple left-to-right beamsearch \n decoder ."}
{"dist": 0.0058430921965842006, "idx": 2, "text": "A surprising example of the power of DNNs is their ability to sort \n N N-bit numbers using only 2 hidden layers of quadratic size [ 27 ] ."}
{"dist": 0.0032286310469205317, "idx": 14, "text": "1 ) ."}
{"dist": 0.0032286310469205317, "idx": 17, "text": "1 ) . \n"}
{"dist": 0.0032286310469205317, "idx": 21, "text": "[ 2 ] ."}
