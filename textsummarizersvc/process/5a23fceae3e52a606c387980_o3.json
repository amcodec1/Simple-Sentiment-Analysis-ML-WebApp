{"dist": 0.035580109089236034, "idx": 1, "text": "However , in live news aggregation sites , this assumption is rarely correct ."}
{"dist": 0.03038982111714267, "idx": 2, "text": "Instead , the events and topics discussed in news stories dramatically change over time ."}
{"dist": 0.029221944701947555, "idx": 4, "text": "We test our system , based on efficiently propagating labels in time-based graphs , with recently published news stories collected over an eighty day period ."}
{"dist": 0.028582755226210937, "idx": 13, "text": "The volume of news stories necessitates the use of an automated classifier ."}
{"dist": 0.028290019184173325, "idx": 15, "text": "Because of the rapidly changing nature of news stories , this may not hold true ."}
{"dist": 0.026402361226659423, "idx": 6, "text": "The writing , vocabulary , and topic of news stories rapidly shift within extremely small periods of time ."}
{"dist": 0.02355637041939635, "idx": 3, "text": "Rather than ignoring this phenomenon , we attempt to explicitly model the transitions of news stories and classifications over time to label stories that may be acquired months after the initial examples are labeled ."}
{"dist": 0.023331971839083063, "idx": 10, "text": "An automated news classifier that is trained to accurately recognize the previous day / month / year \u2019s stories may not have encountered the type of news story that will arise tomorrow . \n"}
{"dist": 0.02203092702043614, "idx": 12, "text": "Instead , we are addressing a related problem of immediate interest to live news aggregation sites : given that a news story has been published , in which of the site \u2019s preset categories should it be placed ? \n"}
{"dist": 0.020183499879069793, "idx": 23, "text": "Section 3 gives a detailed description of the Adsorption algorithm ."}
{"dist": 0.01882962216094679, "idx": 14, "text": "However , one of the fundamental assumptions in machine learning based approaches to news classification is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn ."}
{"dist": 0.018732600871282798, "idx": 24, "text": "Section 4 presents the empirical results to establish the Adsorption baselines for this task ."}
{"dist": 0.01841302372892467, "idx": 19, "text": "Label propagation is gaining popularity in the field of machine learning as a technique for semi- supervised learning ."}
{"dist": 0.017955880272618404, "idx": 16, "text": "In this paper , we present a graph-based \n approach to address the problem of explicitly capturing both strong and weak similarities within news stories over time and to use these efficiently for categorization ."}
{"dist": 0.017303322365659936, "idx": 5, "text": "Experiments presented in this paper include the use of training labels from each story within the first several days of gathering stories , to using a single story as a label . \n"}
{"dist": 0.017020961122578573, "idx": 18, "text": "While Min-Hashing is well-understood in information retrieval applications , our application of it to create a temporal similarity graph appears to be new ."}
{"dist": 0.01692973761089708, "idx": 9, "text": "For example , the \u2018 Political\u2019 and \u2018 Entertainment\u2019 breaking news stories of one week may have very little in common , in terms of subject or even vocabulary , with the news stories of the next week ."}
{"dist": 0.016235823359296648, "idx": 20, "text": "Our approach to label propagation follows our previous work [ 4 ] , where equivalent views of a basic algorithm termed Adsorption were established , and the technique was successfully employed for propagating weak information in extremely large graphs to create a video recommendation system for YouTube . \n"}
{"dist": 0.015188295664154819, "idx": 17, "text": "Our approach combines the paradigm of Min-Hashing and label propagation in graphs in a novel way ."}
{"dist": 0.014512480056023404, "idx": 0, "text": "One of the fundamental assumptions for machine-learning based text classification systems is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn ."}
{"dist": 0.014279968434583972, "idx": 11, "text": "Unlike previous work on topic detection and tracking , we are not attempting to follow a particular topic over time or to determine when a new topic has emerged ."}
{"dist": 0.013816865686485876, "idx": 7, "text": "In recent years , new events and breaking , \u201c hot \u201d , stories almost instantaneously dominate the majority of the press , while older topics just as quickly recede from popularity ."}
{"dist": 0.012363082940020545, "idx": 25, "text": "Section 5 presents extensive results with tiny amounts of labeled data ( e.g. , a single labeled example ) ."}
{"dist": 0.01219355719792869, "idx": 21, "text": "The aims of this paper are to present the following techniques that we anticipate will have general applicability for data mining in industrial settings : formulation of temporal similarities via graphs created using Min-Hashes , and the application of label propagation as an off-the-shelf tool for classification tasks when very little ground truth is available . \n"}
{"dist": 0.007739102176929683, "idx": 8, "text": "For typical automated news-classification systems , this can present severe challenges ."}
{"dist": 0.005881808615730219, "idx": 26, "text": "Section 6 concludes the paper and offers avenues for future exploration ."}
{"dist": 0.005472094325083597, "idx": 22, "text": "The next section describes the data collected and presents a series of experiments to develop strong , realistic , baselines for performance ."}
