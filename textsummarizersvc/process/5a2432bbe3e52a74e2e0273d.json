{"id": "123", "text": "Text summarization problem has many useful applications. If you run a website, you can create titles and short summaries for user generated content. If you want to read a lot of articles and don\u2019t have time to do that, your virtual assistant can summarize main points from these articles for you.\nIt is not an easy problem to solve. There are multiple approaches, including various supervised and unsupervised algorithms. Some algorithms rank the importance of sentences within the text and then construct a summary out of important sentences, others are end-to-end generative models.\nEnd-to-end machine learning algorithms are interesting to try. After all, end-to-end algorithms demonstrate good results in other areas, like image recognition, speech recognition, language translation, and even question-answering.\nText summarization with TensorFlow\n\nIn August 2016, Peter Liu and Xin Pan, software engineers on Google Brain Team, published a blog post \u201cText summarization with TensorFlow\u201d. Their algorithm is extracting interesting parts of the text and create a summary by using these parts of the text and allow for rephrasings to make summary more grammatically correct. This approach is called abstractive summarization.\n\nPeter and Xin trained a text summarization model to produce headlines for news articles, using Annotated English Gigaword, a dataset often used in summarization research. The dataset contains about 10 million documents. The model was trained end-to-end with a deep learning technique called sequence-to-sequence learning.\n\nCode for training and testing the model is included into TensorFlow Models GitHub repository. The core model is a sequence-to-sequence model with attention. When training, the model is using the first two sentences from the article as an input and generates a headline.\n\nWhen decoding, the algorithm is using beam search to find the best headline from candidate headlines generated by the model.\n\nGitHub repository doesn\u2019t include a trained model. The dataset is not publicly available, a license costs $6000 for organizations which are not members of Linguistic Data Consortium. But they include a toy dataset which is enough to run the code.\n\nHow to run\n\nYou will need TensorFlow and Bazel as prerequisites for training the model.\n\nThe toy dataset included into the repository, contains two files in \u201cdata\u201d directory: \u201cdata\u201d and \u201cvocab\u201d. The first one contains a sequence of serialized tensorflow.core.example.example_pb2.Example objects. An example of code to create a file with this format:\n\u201cvocab\u201d file is a text file with the frequency of words in a vocabulary. Each line contains a word, space character and number of occurrences of that word in the dataset. The list is being used to vectorize texts.\nWhen running \u201cdecode\u201d code, note that it will loop over the entire dataset indefinitely, so you will have to stop execution manually at some point. You can find results of decoding in log_root/decode folder. It will contain a few files, some of them have prefix \u201cref\u201d, they contain original headlines from the test set. Other files have prefix \u201cdecode\u201d, they contain headlines generated by the model."}