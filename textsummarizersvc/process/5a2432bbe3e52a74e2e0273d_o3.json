{"dist": 0.042757045578772905, "idx": 8, "text": "Text summarization with TensorFlow"}
{"dist": 0.042757045578772905, "idx": 10, "text": "Text summarization with TensorFlow \u201d ."}
{"dist": 0.03101864367652806, "idx": 0, "text": "Text summarization problem has many useful applications ."}
{"dist": 0.027172233221574058, "idx": 13, "text": "Peter and Xin trained a text summarization model to produce headlines for news articles , using Annotated English Gigaword , a dataset often used in summarization research ."}
{"dist": 0.02099256582304238, "idx": 5, "text": "Some algorithms rank the importance of sentences within the text and then construct a summary out of important sentences , others are end-to-end generative models . \n"}
{"dist": 0.018410001419258814, "idx": 18, "text": "When training , the model is using the first two sentences from the article as an input and generates a headline ."}
{"dist": 0.018383842114730303, "idx": 19, "text": "When decoding , the algorithm is using beam search to find the best headline from candidate headlines generated by the model ."}
{"dist": 0.018083507355717593, "idx": 34, "text": "Other files have prefix \u201c decode \u201d , they contain headlines generated by the model ."}
{"dist": 0.017645110022592735, "idx": 20, "text": "GitHub repository does n\u2019t include a trained model ."}
{"dist": 0.017602789429735102, "idx": 12, "text": "This approach is called abstractive summarization ."}
{"dist": 0.016915852556128846, "idx": 17, "text": "The core model is a sequence-to-sequence model with attention ."}
{"dist": 0.016807887539758124, "idx": 29, "text": "The list is being used to vectorize texts . \n"}
{"dist": 0.016479073396265694, "idx": 16, "text": "Code for training and testing the model is included into TensorFlow Models GitHub repository ."}
{"dist": 0.014684180868159044, "idx": 11, "text": "Their algorithm is extracting interesting parts of the text and create a summary by using these parts of the text and allow for rephrasings to make summary more grammatically correct ."}
{"dist": 0.014375655423191202, "idx": 6, "text": "End-to-end machine learning algorithms are interesting to try ."}
{"dist": 0.013719794698157508, "idx": 15, "text": "The model was trained end-to-end with a deep learning technique called sequence-to-sequence learning ."}
{"dist": 0.01329000784267104, "idx": 24, "text": "You will need TensorFlow and Bazel as prerequisites for training the model ."}
{"dist": 0.013233207315131086, "idx": 7, "text": "After all , end-to-end algorithms demonstrate good results in other areas , like image recognition , speech recognition , language translation , and even question-answering . \n"}
{"dist": 0.011488938068360245, "idx": 4, "text": "There are multiple approaches , including various supervised and unsupervised algorithms ."}
{"dist": 0.011237662084635247, "idx": 22, "text": "But they include a toy dataset which is enough to run the code ."}
{"dist": 0.011209754221000031, "idx": 27, "text": "An example of code to create a file with this format : \n \u201c vocab \u201d file is a text file with the frequency of words in a vocabulary ."}
{"dist": 0.011200004450631944, "idx": 25, "text": "The toy dataset included into the repository , contains two files in \u201c data \u201d directory : \u201c data \u201d and \u201c vocab \u201d ."}
{"dist": 0.010889029903979672, "idx": 14, "text": "The dataset contains about 10 million documents ."}
{"dist": 0.010655897278510702, "idx": 33, "text": "It will contain a few files , some of them have prefix \u201c ref \u201d , they contain original headlines from the test set ."}
{"dist": 0.009467539309386895, "idx": 3, "text": "It is not an easy problem to solve ."}
{"dist": 0.009056622981062492, "idx": 28, "text": "Each line contains a word , space character and number of occurrences of that word in the dataset ."}
{"dist": 0.007942825199245789, "idx": 9, "text": "In August 2016 , Peter Liu and Xin Pan , software engineers on Google Brain Team , published a blog post \u201c"}
{"dist": 0.007940056028111644, "idx": 21, "text": "The dataset is not publicly available , a license costs $ 6000 for organizations which are not members of Linguistic Data Consortium ."}
{"dist": 0.007463728517690414, "idx": 1, "text": "If you run a website , you can create titles and short summaries for user generated content ."}
{"dist": 0.007272178839562503, "idx": 31, "text": "code , note that it will loop over the entire dataset indefinitely , so you will have to stop execution manually at some point ."}
{"dist": 0.006710297265449642, "idx": 2, "text": "If you want to read a lot of articles and do n\u2019t have time to do that , your virtual assistant can summarize main points from these articles for you . \n"}
{"dist": 0.006527863457655579, "idx": 32, "text": "You can find results of decoding in log_root / decode folder ."}
{"dist": 0.00440098878058749, "idx": 26, "text": "The first one contains a sequence of serialized tensorflow.core.example.example_pb2.Example objects ."}
{"dist": 0.004236331765928567, "idx": 30, "text": "When running \u201c decode \u201d"}
{"dist": 0.0024580341461947217, "idx": 23, "text": "How to run"}
